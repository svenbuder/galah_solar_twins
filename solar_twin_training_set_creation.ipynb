{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use selected training set to create input for fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic Tools\n",
    "import numpy as np\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The Cannon\n",
    "# import thecannon as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "galah_elements = [\n",
    "        'Li','C','O',\n",
    "        'Na','Mg','Al','Si',\n",
    "        'K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn',\n",
    "        'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "        'Ba','La','Ce','Nd','Sm','Eu'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Table.read('training_sets/solar_twin_training_set_16xfe_snr50.fits')\n",
    "labels = np.loadtxt('training_sets/solar_twin_training_set_16xfe_snr50_labels',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'log(cm.s**-2)' did not parse as fits unit: 'log' is not a recognized function [astropy.units.core]\n"
     ]
    }
   ],
   "source": [
    "# The download from datacentral also gives a list of sobject_ids for which no spectrum was available\n",
    "# I have renamed them from missing_data_products to be able to identify them\n",
    "sobject_id_without_spectra = np.loadtxt('dr3_spectra/missing_data_products_16xfe_snr50.txt',dtype=str,usecols=(4))\n",
    "sobject_id_without_spectra = [int(sobject_id) for sobject_id in sobject_id_without_spectra]\n",
    "\n",
    "# Let's figure out which ones do not have a spectrum on datacentral\n",
    "exclude = np.ones(len(training_set),dtype=bool)\n",
    "for index, sobject_id in enumerate(training_set['sobject_id']):\n",
    "    if sobject_id in sobject_id_without_spectra:\n",
    "        exclude[index] = False\n",
    "        \n",
    "# There were also spectra with issues while looping through the process\n",
    "problematic_sobject_id = [150827004001397]\n",
    "for sobject_id in problematic_sobject_id:\n",
    "    indices = np.where(sobject_id == training_set['sobject_id'])[0]\n",
    "    if len(indices) > 0:\n",
    "        exclude[indices] = False\n",
    "\n",
    "training_set = training_set[exclude]\n",
    "training_set.write('training_sets/solar_twin_training_set_16xfe_snr50_with_spectra.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a common wavelength grid that we will use for all spectra\n",
    "# This one was used for GALAH DR2\n",
    "wavelengths_for_each_ccd = dict()\n",
    "wavelengths_for_each_ccd['CCD1'] = np.arange(4715.94,4896.00,0.046) # ab lines 4716.3 - 4892.3\n",
    "wavelengths_for_each_ccd['CCD2'] = np.arange(5650.06,5868.25,0.055) # ab lines 5646.0 - 5867.8\n",
    "wavelengths_for_each_ccd['CCD3'] = np.arange(6480.52,6733.92,0.064) # ab lines 6481.6 - 6733.4\n",
    "wavelengths_for_each_ccd['CCD4'] = np.arange(7693.50,7875.55,0.074) # ab lines 7691.2 - 7838.5\n",
    "\n",
    "wavelength_array = np.concatenate(([wavelengths_for_each_ccd['CCD'+ccd] for ccd in ['1','2','3','4']]))\n",
    "\n",
    "wavelength_file = open('training_sets/solar_twin_training_set_16xfe_snr50_wavelength.pickle','wb')\n",
    "pickle.dump((wavelength_array),wavelength_file)\n",
    "wavelength_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a matrix that we will later fill with the normalised flux values\n",
    "normalized_flux = np.ones((np.shape(training_set)[0],np.shape(wavelength)[0]))\n",
    "normalized_ivar = np.ones((np.shape(training_set)[0],np.shape(wavelength)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_normalised_spectra(sobject_id, wavelengths_for_each_ccd, spectrum_path = '/Users/svenbuder/galah_solar_twins/dr3_spectra/hermes'):\n",
    "    \n",
    "    # For each stars, there are 4 spectra for the 4 different CCDs.\n",
    "    # We will interpolate the fluxes and uncertainties/inverse variances onto a common grid\n",
    "    normalised_flux_for_index = []\n",
    "    normalised_ivar_for_index = []\n",
    "    # For that we first interpolate over the individual CCDs\n",
    "    for ccd in ['1','2','3','4']:\n",
    "        ccd_fits = fits.open('dr3_spectra/hermes/'+str(sobject_id)+ccd+'.fits')\n",
    "        # Wavelengths are saved in the headers with CRVAL = starting wavelength and CDELT = delta lambda / pixels\n",
    "        dr3_wavelength = ccd_fits[4].header['CRVAL1'] + ccd_fits[4].header['CDELT1']*np.arange(ccd_fits[4].header['NAXIS1'])\n",
    "        # This is the normalised flux\n",
    "        dr3_normalised_flux = ccd_fits[4].data\n",
    "        # Uncertainties are saved on a relative scale in extension 0\n",
    "        # Combining them with the normalised flux gives the standard deviation *sigma*\n",
    "        dr3_normalised_error = ccd_fits[1].data * ccd_fits[4].data\n",
    "        ccd_fits.close()\n",
    "        \n",
    "        # Interpolate from DR3 wavelength onto our common wavelength\n",
    "        interpolated_normalised_flux = np.interp(\n",
    "            x  = wavelengths_for_each_ccd['CCD'+ccd],\n",
    "            xp = dr3_wavelength,\n",
    "            fp = dr3_normalised_flux\n",
    "        )\n",
    "        normalised_flux_for_index.append(interpolated_normalised_flux)\n",
    "\n",
    "        # Interpolate from DR3 wavelength onto our common wavelength\n",
    "        interpolated_normalised_error = np.interp(\n",
    "            x  = wavelengths_for_each_ccd['CCD'+ccd],\n",
    "            xp = dr3_wavelength,\n",
    "            fp = dr3_normalised_error\n",
    "        )\n",
    "        # instead of standard deviation, add 1/var == 1/std**2\n",
    "        normalised_ivar_for_index.append(1./(interpolated_normalised_error)**2)\n",
    "    \n",
    "    normalised_flux_for_index = np.concatenate((normalised_flux_for_index))\n",
    "    normalised_ivar_for_index = np.concatenate((normalised_ivar_for_index))\n",
    "    \n",
    "    return(normalised_flux_for_index,normalised_ivar_for_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def populate_normalised_flux_and_ivar_matrix(matrix_index, wavelengths_for_each_ccd):\n",
    "    sobject_id = training_set['sobject_id'][matrix_index]\n",
    "    try:\n",
    "        normalised_flux_for_index, normalised_ivar_for_index = load_normalised_spectra(sobject_id,wavelengths_for_each_ccd=wavelengths_for_each_ccd)\n",
    "    except:\n",
    "        print('Failed to load spectrum for index '+str(matrix_index)+', that is, sobject_id '+str(sobject_id))\n",
    "    normalized_flux[matrix_index] = normalised_flux_for_index\n",
    "    normalized_ivar[matrix_index] = normalised_ivar_for_index\n",
    "\n",
    "# let's iterate over all matrix indices now\n",
    "[populate_normalised_flux_and_ivar_matrix(matrix_index=index, wavelengths_for_each_ccd=wavelengths_for_each_ccd) for index in range(np.shape(training_set)[0])];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_matrix(wavelength_array,labels):\n",
    "    \n",
    "    # First create a matrix, where all pixels are unmasked == 1 (used for fitting)\n",
    "    mask_matrix = np.ones((np.shape(labels)[0],np.shape(wavelength_array)[0]))\n",
    "    \n",
    "    \"\"\"\n",
    "    To Do:\n",
    "    - neglect cores of Halpha and Hbeta for stellar parameters\n",
    "    - only use lines from Hinkle Atlas to avoid lines of element X being associated with element Y\n",
    "    \n",
    "    # Now for each label, figure out, which wavelength pixels we want to activate for fitting\n",
    "    for label_index, label in enumerate(labels):\n",
    "        if label in ['teff','logg','fe_h','vbroad']:\n",
    "            mask_matrix[:,label_index] = 1\n",
    "        if label in galah_elements:\n",
    "            mask_matrix[:,label_index] = ... array with 1 for line region and 0 outside\n",
    "    \"\"\"\n",
    "    return mask_matrix\n",
    "mask_matrix = create_mask_matrix(wavelength_array,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_ivar_file = open('training_sets/solar_twin_training_set_16xfe_snr50_flux_ivar.pickle','wb')\n",
    "pickle.dump((normalized_flux,normalized_ivar),flux_ivar_file)\n",
    "flux_ivar_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
